# -*- coding: utf-8 -*-
"""FINAL_PROJECT_RNN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hv5Lq9oIrxNBS9nvhSW3DRrnSWMrX4U_
"""

# =========================
# 1. Imports & Setup
# =========================
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error
from xgboost import XGBRegressor
import warnings
warnings.filterwarnings("ignore")

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# =========================
# 2. Load Dataset
# =========================
df = pd.read_csv("/content/household_power_consumption.csv")

df.columns = df.columns.str.strip().str.lower()
df["time"] = pd.to_datetime(df["time"], errors="coerce")
df = df.set_index("time")

df = df.apply(pd.to_numeric, errors="coerce")
df = df.interpolate(method="time")

# =========================
# 3. Feature Engineering
# =========================
df["hour"] = df.index.hour
df["dayofweek"] = df.index.dayofweek
df["rolling_mean_24"] = df["global_active_power"].rolling(24).mean()
df["rolling_std_24"] = df["global_active_power"].rolling(24).std()
df = df.dropna()

features = [
    "global_active_power",
    "global_reactive_power",
    "voltage",
    "global_intensity",
    "sub_metering_1",
    "sub_metering_2",
    "sub_metering_3",
    "hour",
    "dayofweek",
    "rolling_mean_24",
    "rolling_std_24"
]

# =========================
# 4. Scaling
# =========================
scaler = StandardScaler()
scaled_data = scaler.fit_transform(df[features])

# =========================
# 5. Sequence Creation
# =========================
def create_sequences(data, seq_len, target_col=0):
    X, y = [], []
    for i in range(len(data) - seq_len):
        X.append(data[i:i + seq_len])
        y.append(data[i + seq_len, target_col])
    return np.array(X), np.array(y)

SEQ_LEN = 48
X, y = create_sequences(scaled_data, SEQ_LEN)

# =========================
# 6. Rolling Window Split
# =========================
def rolling_window_split(X, y, window=5000, horizon=1000):
    splits = []
    start = 0
    while start + window + horizon < len(X):
        splits.append((
            X[start:start + window],
            y[start:start + window],
            X[start + window:start + window + horizon],
            y[start + window:start + window + horizon]
        ))
        start += horizon
    return splits

splits = rolling_window_split(X, y)
X_tr, y_tr, X_val, y_val = splits[0]

# =========================
# 7. Dataset
# =========================
class TimeSeriesDataset(Dataset):
    def __init__(self, X, y):
        self.X = torch.tensor(X, dtype=torch.float32)
        self.y = torch.tensor(y, dtype=torch.float32)

    def __len__(self):
        return len(self.X)

    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]

# =========================
# 8. Transformer with REAL Attention
# =========================
class TransformerForecast(nn.Module):
    def __init__(self, input_dim, d_model, nhead, num_layers):
        super().__init__()

        self.embedding = nn.Linear(input_dim, d_model)

        self.attn_layers = nn.ModuleList([
            nn.MultiheadAttention(
                embed_dim=d_model,
                num_heads=nhead,
                batch_first=True
            ) for _ in range(num_layers)
        ])

        self.norms = nn.ModuleList([
            nn.LayerNorm(d_model) for _ in range(num_layers)
        ])

        self.ffns = nn.ModuleList([
            nn.Sequential(
                nn.Linear(d_model, d_model * 4),
                nn.ReLU(),
                nn.Linear(d_model * 4, d_model)
            ) for _ in range(num_layers)
        ])

        self.fc = nn.Linear(d_model, 1)
        self.attention_weights = None

    def forward(self, x):
        x = self.embedding(x)

        for attn, norm, ffn in zip(self.attn_layers, self.norms, self.ffns):
            attn_out, attn_weights = attn(x, x, x, need_weights=True)
            self.attention_weights = attn_weights
            x = norm(x + attn_out)
            x = norm(x + ffn(x))

        return self.fc(x[:, -1, :]).squeeze()

# =========================
# 9. Train Transformer
# =========================
configs = [
    {"d_model": 32, "heads": 2},
    {"d_model": 64, "heads": 4}
]

best_rmse = float("inf")
best_model = None

for cfg in configs:
    model = TransformerForecast(
        input_dim=X.shape[2],
        d_model=cfg["d_model"],
        nhead=cfg["heads"],
        num_layers=2
    ).to(device)

    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    criterion = nn.MSELoss()

    loader = DataLoader(
        TimeSeriesDataset(X_tr, y_tr),
        batch_size=64,
        shuffle=True
    )

    model.train()
    for _ in range(3):
        for xb, yb in loader:
            xb, yb = xb.to(device), yb.to(device)
            optimizer.zero_grad()
            loss = criterion(model(xb), yb)
            loss.backward()
            optimizer.step()

    model.eval()
    with torch.no_grad():
        preds = model(torch.tensor(X_val, dtype=torch.float32).to(device)).cpu().numpy()

    rmse = np.sqrt(mean_squared_error(y_val, preds))
    print(f"{cfg} RMSE: {rmse:.4f}")

    if rmse < best_rmse:
        best_rmse = rmse
        best_model = model

print("Best RMSE:", best_rmse)

# =========================
# ðŸ”Ÿ Attention Analysis (WORKS)
# =========================
with torch.no_grad():
    _ = best_model(torch.tensor(X_val[:64], dtype=torch.float32).to(device))
    attention = best_model.attention_weights.cpu().numpy()
    # shape: (batch, heads, seq_len, seq_len)

    timestep_importance = attention.mean(axis=(0, 1, 2))
    print("Top 5 important timesteps:",
          np.argsort(timestep_importance)[-5:])

# =========================
# 1ï¸âƒ£1ï¸âƒ£ XGBoost Baseline (FIXED)
# =========================
lag_df = pd.DataFrame(index=df.index)

for col in features:
    lag_df[f"{col}_lag1"] = df[col].shift(1)
    lag_df[f"{col}_lag24"] = df[col].shift(24)

lag_df = lag_df.dropna()
target = df["global_active_power"].loc[lag_df.index]

split = int(0.8 * len(lag_df))

xgb = XGBRegressor(
    n_estimators=300,
    max_depth=6,
    learning_rate=0.05
)

xgb.fit(lag_df[:split], target[:split])

# =========================
# 1ï¸âƒ£2ï¸âƒ£ Save Model
# =========================
torch.save(best_model.state_dict(), "final_transformer_attention_model.pth")
print("Model saved successfully")